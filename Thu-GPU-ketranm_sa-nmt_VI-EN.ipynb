{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Thu-GPU-ketranm_sa-nmt.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WJ0h9EUrHyVI","colab_type":"text"},"source":["To run this notebook, follow the steps below.\n","\n","1. cd to Code/sa-nmt diretcory\n","\n","2. train first, don't clear the output, watch for BLEU score (appears every 5000 steps) and acc (accuracy)\n","\n","3. when file model_best.pt appears in sa-nmt/ folder (every 5000 train steps), we can use it to translate\n","\n","4. translate by\n","  \n","  4.1 create a source sentences, write it in file input.txt (cell has %% writefile input.txt)\n","  \n","  4.2 run translate command in the cell\n","  \n","  4.3 see result by viewing file trans.bpe (cell has %pycat trans.bpe)\n","\n","NOTE: if path error occurs, Restart Runtime and do again from cell 1"]},{"cell_type":"code","metadata":{"id":"_qUmvlahZmq-","colab_type":"code","outputId":"6fe6ce9e-6046-4b10-f590-3d48939d30a1","executionInfo":{"status":"ok","timestamp":1582008582728,"user_tz":-420,"elapsed":47385,"user":{"displayName":"Thư Trần Thị Anh","photoUrl":"","userId":"06893536815286384763"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["# step 1\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lp29ktj4O10M","colab_type":"code","colab":{}},"source":["# step 2.a:\n","# Thu:\n","import os\n","os.chdir(\"drive/My Drive/_Schoolings/CS418 - Introduction to Natural Language Processing/Final Present/nlp_gpu_sa_nmt/Code/sa-nmt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJrICHBqO7z8","colab_type":"code","colab":{}},"source":["# step 2.b:\n","# Trinh: ENG-VN\n","# import os\n","# os.chdir(\"drive/My Drive/NLP/sa-nmt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZMu63VE9EJKG","colab_type":"code","colab":{}},"source":["# step 2.c:\n","# Phuc: VN-ENG\n","# import os\n","# os.chdir(\"drive/My Drive/nlp_gpu_sa_nmt/Code/sa-nmt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"irn9N08-QsZI","colab_type":"code","outputId":"22c5d08e-f3d5-4532-c2f0-1334c49fe679","executionInfo":{"status":"ok","timestamp":1577098284220,"user_tz":-420,"elapsed":8106,"user":{"displayName":"Phúc Trần Bảo","photoUrl":"","userId":"04637064817265378505"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["!pwd\n","!ls\n","!python -V"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content\n","drive  sample_data\n","Python 3.6.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jmXO-JzgmI12","colab_type":"code","outputId":"2624146f-7b69-4144-9d14-19b11842ed59","executionInfo":{"status":"ok","timestamp":1578409634444,"user_tz":-420,"elapsed":8083,"user":{"displayName":"Phúc Trần Bảo","photoUrl":"","userId":"04637064817265378505"}},"colab":{"base_uri":"https://localhost:8080/","height":390}},"source":["!nvidia-smi\n","!nvcc --version"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tue Jan  7 15:07:10 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   45C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2018 NVIDIA Corporation\n","Built on Sat_Aug_25_21:08:01_CDT_2018\n","Cuda compilation tools, release 10.0, V10.0.130\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vEuUzWK52mim","colab_type":"code","colab":{}},"source":["# !!python train.py -datasets iwslt/train.en-de.de.tok.bpe iwslt/train.en-de.en.tok.bpe -valid_datasets iwslt/tst2015.en-de.de.tok.bpe iwslt/tst2015.en-de.en.tok.bpe -dicts iwslt/train.en-de.de.tok.bpe.pkl iwslt/train.en-de.en.tok.bpe.pkl -share_decoder_embeddings -word_vec_size 32 -rnn_size 32 -encoder_type sabrnn -encode_multi_key -share_attn -report_every 100 -learning_rate 0.001 -eval_every 5000 -epochs 13 -batch_size 64 -max_updates 1000000 -gpuid 0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ulv_Dj_KErTw","colab_type":"text"},"source":["TRAIN, need to keep colab running continuously long until reaching iteration 1 mil/1 mil. \n","Google Colab will disconnect sooner, but it's ok, try to keep it running as long as possible\n","# **TRAIN: VN-ENG**"]},{"cell_type":"markdown","metadata":{"id":"KjK607y51BYN","colab_type":"text"},"source":["-train_from model_best.pt\n","-share_attn"]},{"cell_type":"code","metadata":{"id":"q3ZnqY5MkSmH","colab_type":"code","outputId":"535511b6-1555-4f97-9947-9046697d81a8","executionInfo":{"status":"ok","timestamp":1581960759078,"user_tz":-420,"elapsed":18440985,"user":{"displayName":"Thư Trần Thị Anh","photoUrl":"","userId":"06893536815286384763"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python train.py -datasets iwslt/train.vi.bpe iwslt/train.en.bpe -valid_datasets iwslt/test2015.vi.bpe iwslt/test2015.en.bpe -dicts iwslt/all.vi.bpe.pkl iwslt/all.en.bpe.pkl -share_decoder_embeddings -word_vec_size 1024 -rnn_size 1024 -encoder_type sabrnn -encode_multi_key -share_attn -report_every 100 -learning_rate 0.001 -eval_every 1000 -epochs 13 -batch_size 32 -max_updates 1000000 -gpuid 0 -layers 4 -dropout 0.5"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Namespace(batch_size=32, beam_size=12, datasets=['iwslt/train.vi.bpe', 'iwslt/train.en.bpe'], decoder_type='rnn', dicts=['iwslt/all.vi.bpe.pkl', 'iwslt/all.en.bpe.pkl'], dropout=0.5, encode_multi_key=True, encoder_type='sabrnn', epochs=13, eval_every=1000, gpuid=[0], hard=False, input_feed=1, layers=4, learning_rate=0.001, learning_rate_decay=0.5, max_decay_step=5, max_generator_batches=32, max_grad_norm=5, max_seq_length=100, max_thres=7.0, max_updates=1000000, min_thres=-5.0, optim='adam', param_init=0.1, report_every=100, rnn_size=1024, rnn_type='LSTM', save_model='model', seed=42, share_attn=True, share_decoder_embeddings=True, shuffle=1, src_vocab_size=-1, start_checkpoint_at=0, start_epoch=1, start_eval_checkpoint_at=0, tgt_vocab_size=-1, train_from='', valid_datasets=['iwslt/test2015.vi.bpe', 'iwslt/test2015.en.bpe'], word_vec_size=1024)\n","| build data iterators\n","shuffle iwslt/train.vi.bpe | iwslt/train.en.bpe\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","| vocabulary size. source = 9759; target = 9973\n","/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n","Add punctuation constrain!\n","NMT(\n","  (encoder): SAEncoder(\n","    (embeddings): Embedding(9759, 1024, padding_idx=0)\n","    (rnn): LSTM(1024, 512, num_layers=4, dropout=0.5, bidirectional=True)\n","    (tree_attn): TreeAttention(\n","      (q): Linear(in_features=1024, out_features=1024, bias=False)\n","      (k): Linear(in_features=1024, out_features=1024, bias=False)\n","      (v): Linear(in_features=1024, out_features=1024, bias=False)\n","      (dtree): MatrixTree()\n","    )\n","  )\n","  (decoder): Decoder(\n","    (embeddings): Embedding(9973, 1024, padding_idx=0)\n","    (rnn): StackedLSTM(\n","      (dropout): Dropout(p=0.5, inplace=False)\n","      (layers): ModuleList(\n","        (0): LSTMCell(2048, 1024)\n","        (1): LSTMCell(1024, 1024)\n","        (2): LSTMCell(1024, 1024)\n","        (3): LSTMCell(1024, 1024)\n","      )\n","    )\n","    (attn): GlobalAttention(\n","      (linear_in): Linear(in_features=1024, out_features=1024, bias=False)\n","      (linear_out): Linear(in_features=3072, out_features=1024, bias=False)\n","      (gate): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=1024, out_features=9973, bias=True)\n","    (1): LogSoftmax()\n","  )\n",")\n","* number of parameters: 91586293\n","encoder:  38338560\n","decoder:  53247733\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","526081.3949327469 74572\n","Epoch  1,   100/1000000; acc:   4.94; ppl: 1158.26; 1599 tgt tok/s;     47 s elapsed\n","476417.5703253746 73028\n","Epoch  1,   200/1000000; acc:   5.94; ppl: 681.14; 1587 tgt tok/s;     93 s elapsed\n","473460.022567749 73828\n","Epoch  1,   300/1000000; acc:   6.59; ppl: 609.73; 1595 tgt tok/s;    139 s elapsed\n","478981.0436325073 75337\n","Epoch  1,   400/1000000; acc:   6.79; ppl: 577.00; 1614 tgt tok/s;    186 s elapsed\n","469161.295671463 74479\n","Epoch  1,   500/1000000; acc:   7.67; ppl: 544.16; 1609 tgt tok/s;    232 s elapsed\n","471058.2045240402 75066\n","Epoch  1,   600/1000000; acc:   7.72; ppl: 531.26; 1619 tgt tok/s;    278 s elapsed\n","455831.322892189 73317\n","Epoch  1,   700/1000000; acc:   8.61; ppl: 501.33; 1597 tgt tok/s;    324 s elapsed\n","459745.1947007179 74322\n","Epoch  1,   800/1000000; acc:   9.85; ppl: 485.83; 1601 tgt tok/s;    371 s elapsed\n","443839.84257507324 72966\n","Epoch  1,   900/1000000; acc:  11.22; ppl: 438.27; 1588 tgt tok/s;    417 s elapsed\n","447496.665102005 74476\n","Epoch  1,  1000/1000000; acc:  12.63; ppl: 406.91; 1604 tgt tok/s;    463 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","133976.83308410645 22499\n","133976.83308410645 22499\n","Validation perplexity 1000: 385.596\n","Learning rate: 0.001\n","/content/drive/My Drive/_Schoolings/CS418 - Introduction to Natural Language Processing/Final Present/nlp_gpu_sa_nmt/Code/sa-nmt/infer.py:37: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  volatile=True)\n","/content/drive/My Drive/_Schoolings/CS418 - Introduction to Natural Language Processing/Final Present/nlp_gpu_sa_nmt/Code/sa-nmt/infer.py:64: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  init_target = Variable(init_target, volatile=True)\n","/content/drive/My Drive/_Schoolings/CS418 - Introduction to Natural Language Processing/Final Present/nlp_gpu_sa_nmt/Code/sa-nmt/infer.py:115: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  init_target = Variable(next_ys.view(1, -1), volatile=True)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 0.41, 21.5/1.8/0.2/0.0 (BP=0.706, ratio=0.742, hyp_len=15462, ref_len=20850)\n","\n","Validation BLEU 1000: 0.41\n","Saved model: 1000 | BLEU 0.41\n","423254.9300189018 74198\n","Epoch  1,  1100/1000000; acc:  18.71; ppl: 300.18; 418 tgt tok/s;    641 s elapsed\n","409803.47062397003 75199\n","Epoch  1,  1200/1000000; acc:  21.47; ppl: 232.66; 1613 tgt tok/s;    687 s elapsed\n","393816.7436494827 74729\n","Epoch  1,  1300/1000000; acc:  23.06; ppl: 194.40; 1592 tgt tok/s;    734 s elapsed\n","382989.0197005272 75054\n","Epoch  1,  1400/1000000; acc:  24.39; ppl: 164.49; 1614 tgt tok/s;    781 s elapsed\n","372844.02834415436 75312\n","Epoch  1,  1500/1000000; acc:  25.85; ppl: 141.27; 1606 tgt tok/s;    828 s elapsed\n","367540.99871730804 75069\n","Epoch  1,  1600/1000000; acc:  26.43; ppl: 133.76; 1616 tgt tok/s;    874 s elapsed\n","361351.57728099823 74998\n","Epoch  1,  1700/1000000; acc:  27.50; ppl: 123.74; 1611 tgt tok/s;    921 s elapsed\n","355069.2197971344 75371\n","Epoch  1,  1800/1000000; acc:  28.34; ppl: 111.16; 1614 tgt tok/s;    967 s elapsed\n","339053.5930042267 73247\n","Epoch  1,  1900/1000000; acc:  29.10; ppl: 102.40; 1586 tgt tok/s;   1013 s elapsed\n","328764.89341259 72231\n","Epoch  1,  2000/1000000; acc:  30.54; ppl:  94.78; 1575 tgt tok/s;   1059 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","103012.07653808594 22499\n","103012.07653808594 22499\n","Validation perplexity 2000: 97.37\n","Learning rate: 0.001\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 5.78, 34.4/9.4/3.1/1.1 (BP=1.000, ratio=1.040, hyp_len=21674, ref_len=20850)\n","\n","Validation BLEU 2000: 5.78\n","Saved model: 2000 | BLEU 5.78\n","339402.434841156 75332\n","Epoch  1,  2100/1000000; acc:  30.56; ppl:  90.51; 388 tgt tok/s;   1253 s elapsed\n","316585.6295938492 72007\n","Epoch  1,  2200/1000000; acc:  31.93; ppl:  81.17; 1566 tgt tok/s;   1299 s elapsed\n","327811.0608139038 74702\n","Epoch  1,  2300/1000000; acc:  31.88; ppl:  80.50; 1606 tgt tok/s;   1346 s elapsed\n","320130.67446136475 73968\n","Epoch  1,  2400/1000000; acc:  32.57; ppl:  75.79; 1603 tgt tok/s;   1392 s elapsed\n","318300.1675682068 74231\n","Epoch  1,  2500/1000000; acc:  33.02; ppl:  72.82; 1597 tgt tok/s;   1438 s elapsed\n","311746.42463207245 73548\n","Epoch  1,  2600/1000000; acc:  33.74; ppl:  69.32; 1596 tgt tok/s;   1484 s elapsed\n","318887.48959970474 76335\n","Epoch  1,  2700/1000000; acc:  34.24; ppl:  65.20; 1627 tgt tok/s;   1531 s elapsed\n","311770.70365428925 74968\n","Epoch  1,  2800/1000000; acc:  34.31; ppl:  63.99; 1608 tgt tok/s;   1578 s elapsed\n","301132.5862374306 73577\n","Epoch  1,  2900/1000000; acc:  35.02; ppl:  59.90; 1600 tgt tok/s;   1624 s elapsed\n","300217.36679410934 74547\n","Epoch  1,  3000/1000000; acc:  35.95; ppl:  56.10; 1615 tgt tok/s;   1670 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","91478.99584960938 22499\n","91478.99584960938 22499\n","Validation perplexity 3000: 58.3182\n","Learning rate: 0.001\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 9.01, 37.5/12.9/5.4/2.5 (BP=1.000, ratio=1.137, hyp_len=23715, ref_len=20850)\n","\n","Validation BLEU 3000: 9.01\n","Saved model: 3000 | BLEU 9.01\n","308838.523607254 75825\n","Epoch  1,  3100/1000000; acc:  35.61; ppl:  58.74; 381 tgt tok/s;   1869 s elapsed\n","292315.0753417015 73471\n","Epoch  1,  3200/1000000; acc:  36.26; ppl:  53.44; 1599 tgt tok/s;   1915 s elapsed\n","292318.00642061234 74376\n","Epoch  1,  3300/1000000; acc:  36.78; ppl:  50.92; 1600 tgt tok/s;   1962 s elapsed\n","295436.6421766281 75071\n","Epoch  1,  3400/1000000; acc:  36.89; ppl:  51.18; 1610 tgt tok/s;   2008 s elapsed\n","289238.83889341354 74662\n","Epoch  1,  3500/1000000; acc:  37.36; ppl:  48.13; 1606 tgt tok/s;   2055 s elapsed\n","shuffle iwslt/train.vi.bpe | iwslt/train.en.bpe\n","27694.224500656128 6756\n","Epoch  2,  3600/1000000; acc:  33.53; ppl:  60.29; 1875 tgt tok/s;      4 s elapsed\n","274607.3327140808 73525\n","Epoch  2,  3700/1000000; acc:  38.66; ppl:  41.88; 1595 tgt tok/s;     50 s elapsed\n","275747.98149871826 74396\n","Epoch  2,  3800/1000000; acc:  39.06; ppl:  40.71; 1605 tgt tok/s;     96 s elapsed\n","271514.81200504303 73206\n","Epoch  2,  3900/1000000; acc:  39.08; ppl:  40.81; 1596 tgt tok/s;    142 s elapsed\n","279466.59722423553 75353\n","Epoch  2,  4000/1000000; acc:  39.11; ppl:  40.80; 1612 tgt tok/s;    189 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","84616.49770355225 22499\n","84616.49770355225 22499\n","Validation perplexity 4000: 42.9871\n","Learning rate: 0.001\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 12.21, 43.5/16.8/7.8/3.9 (BP=1.000, ratio=1.026, hyp_len=21399, ref_len=20850)\n","\n","Validation BLEU 4000: 12.21\n","Saved model: 4000 | BLEU 12.21\n","269649.5066795349 73781\n","Epoch  2,  4100/1000000; acc:  39.67; ppl:  38.66; 384 tgt tok/s;    381 s elapsed\n","274697.0373735428 75223\n","Epoch  2,  4200/1000000; acc:  39.60; ppl:  38.54; 1601 tgt tok/s;    428 s elapsed\n","268207.63087558746 74128\n","Epoch  2,  4300/1000000; acc:  40.01; ppl:  37.27; 1591 tgt tok/s;    474 s elapsed\n","268695.88248062134 74860\n","Epoch  2,  4400/1000000; acc:  40.28; ppl:  36.21; 1612 tgt tok/s;    521 s elapsed\n","260441.62524604797 72353\n","Epoch  2,  4500/1000000; acc:  40.15; ppl:  36.58; 1579 tgt tok/s;    567 s elapsed\n","270282.32605600357 75346\n","Epoch  2,  4600/1000000; acc:  40.44; ppl:  36.13; 1614 tgt tok/s;    613 s elapsed\n","268365.44447374344 74922\n","Epoch  2,  4700/1000000; acc:  40.54; ppl:  35.94; 1614 tgt tok/s;    660 s elapsed\n","269719.19955444336 75548\n","Epoch  2,  4800/1000000; acc:  40.76; ppl:  35.52; 1617 tgt tok/s;    707 s elapsed\n","262941.3912768364 74725\n","Epoch  2,  4900/1000000; acc:  40.96; ppl:  33.74; 1591 tgt tok/s;    754 s elapsed\n","257523.8455119133 73612\n","Epoch  2,  5000/1000000; acc:  41.36; ppl:  33.06; 1583 tgt tok/s;    800 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","79266.37633514404 22499\n","79266.37633514404 22499\n","Validation perplexity 5000: 33.8895\n","Learning rate: 0.001\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 14.49, 45.3/19.3/9.6/5.2 (BP=1.000, ratio=1.063, hyp_len=22167, ref_len=20850)\n","\n","Validation BLEU 5000: 14.49\n","Saved model: 5000 | BLEU 14.49\n","262098.7650346756 74606\n","Epoch  2,  5100/1000000; acc:  41.16; ppl:  33.55; 387 tgt tok/s;    993 s elapsed\n","266453.8663172722 76326\n","Epoch  2,  5200/1000000; acc:  41.52; ppl:  32.82; 1624 tgt tok/s;   1040 s elapsed\n","255781.79904270172 74140\n","Epoch  2,  5300/1000000; acc:  41.84; ppl:  31.50; 1604 tgt tok/s;   1086 s elapsed\n","253452.39473581314 73852\n","Epoch  2,  5400/1000000; acc:  41.99; ppl:  30.94; 1592 tgt tok/s;   1133 s elapsed\n","265032.5475668907 76350\n","Epoch  2,  5500/1000000; acc:  41.35; ppl:  32.18; 1618 tgt tok/s;   1180 s elapsed\n","256927.14600086212 75111\n","Epoch  2,  5600/1000000; acc:  42.53; ppl:  30.59; 1602 tgt tok/s;   1227 s elapsed\n","255777.89140367508 75163\n","Epoch  2,  5700/1000000; acc:  42.48; ppl:  30.05; 1598 tgt tok/s;   1274 s elapsed\n","246929.0386505127 73737\n","Epoch  2,  5800/1000000; acc:  43.10; ppl:  28.47; 1588 tgt tok/s;   1320 s elapsed\n","250650.31569576263 74108\n","Epoch  2,  5900/1000000; acc:  42.55; ppl:  29.44; 1596 tgt tok/s;   1367 s elapsed\n","246490.53123664856 74046\n","Epoch  2,  6000/1000000; acc:  43.06; ppl:  27.91; 1587 tgt tok/s;   1413 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","76453.84957885742 22499\n","76453.84957885742 22499\n","Validation perplexity 6000: 29.9072\n","Learning rate: 0.001\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 15.42, 46.3/20.4/10.5/5.7 (BP=1.000, ratio=1.092, hyp_len=22763, ref_len=20850)\n","\n","Validation BLEU 6000: 15.42\n","Saved model: 6000 | BLEU 15.42\n","245454.46166324615 73195\n","Epoch  2,  6100/1000000; acc:  42.93; ppl:  28.60; 384 tgt tok/s;   1604 s elapsed\n","244740.07674980164 74361\n","Epoch  2,  6200/1000000; acc:  43.56; ppl:  26.88; 1623 tgt tok/s;   1650 s elapsed\n","249725.95414066315 74669\n","Epoch  2,  6300/1000000; acc:  42.88; ppl:  28.34; 1606 tgt tok/s;   1696 s elapsed\n","251012.5766568184 75685\n","Epoch  2,  6400/1000000; acc:  43.45; ppl:  27.56; 1612 tgt tok/s;   1743 s elapsed\n","243097.29250335693 74011\n","Epoch  2,  6500/1000000; acc:  43.64; ppl:  26.70; 1599 tgt tok/s;   1789 s elapsed\n","246298.48024606705 74371\n","Epoch  2,  6600/1000000; acc:  43.45; ppl:  27.43; 1603 tgt tok/s;   1836 s elapsed\n","242778.71662521362 73864\n","Epoch  2,  6700/1000000; acc:  43.67; ppl:  26.76; 1607 tgt tok/s;   1882 s elapsed\n","241164.0378727913 73647\n","Epoch  2,  6800/1000000; acc:  43.55; ppl:  26.43; 1605 tgt tok/s;   1928 s elapsed\n","245803.66757202148 75030\n","Epoch  2,  6900/1000000; acc:  43.85; ppl:  26.47; 1613 tgt tok/s;   1974 s elapsed\n","240801.57524108887 73780\n","Epoch  2,  7000/1000000; acc:  43.84; ppl:  26.15; 1597 tgt tok/s;   2020 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","73867.21101760864 22499\n","73867.21101760864 22499\n","Validation perplexity 7000: 26.6592\n","Learning rate: 0.001\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 16.68, 48.5/22.0/11.5/6.3 (BP=1.000, ratio=1.044, hyp_len=21762, ref_len=20850)\n","\n","Validation BLEU 7000: 16.68\n","Saved model: 7000 | BLEU 16.68\n","241018.82773208618 74173\n","Epoch  2,  7100/1000000; acc:  43.78; ppl:  25.78; 392 tgt tok/s;   2209 s elapsed\n","shuffle iwslt/train.vi.bpe | iwslt/train.en.bpe\n","37381.582973480225 11222\n","Epoch  3,  7200/1000000; acc:  42.28; ppl:  27.97; 1846 tgt tok/s;      6 s elapsed\n","228085.2503194809 74407\n","Epoch  3,  7300/1000000; acc:  45.66; ppl:  21.44; 1612 tgt tok/s;     52 s elapsed\n","224913.89687013626 73429\n","Epoch  3,  7400/1000000; acc:  45.67; ppl:  21.39; 1597 tgt tok/s;     98 s elapsed\n","234010.91022062302 75101\n","Epoch  3,  7500/1000000; acc:  45.08; ppl:  22.55; 1629 tgt tok/s;    144 s elapsed\n","230788.15647220612 74709\n","Epoch  3,  7600/1000000; acc:  45.44; ppl:  21.96; 1609 tgt tok/s;    191 s elapsed\n","227495.94849967957 73695\n","Epoch  3,  7700/1000000; acc:  45.68; ppl:  21.91; 1595 tgt tok/s;    237 s elapsed\n","223414.97209453583 73120\n","Epoch  3,  7800/1000000; acc:  45.81; ppl:  21.23; 1595 tgt tok/s;    283 s elapsed\n","228401.71730804443 73933\n","Epoch  3,  7900/1000000; acc:  45.14; ppl:  21.96; 1592 tgt tok/s;    329 s elapsed\n","224222.30261325836 73543\n","Epoch  3,  8000/1000000; acc:  45.99; ppl:  21.09; 1574 tgt tok/s;    376 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","71970.89912414551 22499\n","71970.89912414551 22499\n","Validation perplexity 8000: 24.5043\n","Learning rate: 0.001\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 17.52, 48.6/22.8/12.2/7.0 (BP=1.000, ratio=1.066, hyp_len=22220, ref_len=20850)\n","\n","Validation BLEU 8000: 17.52\n","Saved model: 8000 | BLEU 17.52\n","230682.04955291748 74919\n","Epoch  3,  8100/1000000; acc:  45.43; ppl:  21.74; 390 tgt tok/s;    568 s elapsed\n","228985.29138708115 74980\n","Epoch  3,  8200/1000000; acc:  45.59; ppl:  21.20; 1619 tgt tok/s;    615 s elapsed\n","225436.70090961456 73847\n","Epoch  3,  8300/1000000; acc:  45.95; ppl:  21.17; 1588 tgt tok/s;    661 s elapsed\n","228219.75154829025 75009\n","Epoch  3,  8400/1000000; acc:  46.16; ppl:  20.96; 1609 tgt tok/s;    708 s elapsed\n","229378.05580472946 75540\n","Epoch  3,  8500/1000000; acc:  46.06; ppl:  20.83; 1614 tgt tok/s;    754 s elapsed\n","225335.7589454651 74102\n","Epoch  3,  8600/1000000; acc:  46.17; ppl:  20.92; 1589 tgt tok/s;    801 s elapsed\n","226937.25032043457 74817\n","Epoch  3,  8700/1000000; acc:  46.20; ppl:  20.76; 1599 tgt tok/s;    848 s elapsed\n","226288.24913406372 74628\n","Epoch  3,  8800/1000000; acc:  46.23; ppl:  20.74; 1609 tgt tok/s;    894 s elapsed\n","227862.16271209717 74762\n","Epoch  3,  8900/1000000; acc:  45.89; ppl:  21.07; 1612 tgt tok/s;    941 s elapsed\n","228431.24726104736 75685\n","Epoch  3,  9000/1000000; acc:  46.23; ppl:  20.45; 1617 tgt tok/s;    987 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","69930.51947784424 22499\n","69930.51947784424 22499\n","Validation perplexity 9000: 22.3799\n","Learning rate: 0.001\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 18.90, 51.0/24.3/13.3/7.7 (BP=1.000, ratio=1.029, hyp_len=21450, ref_len=20850)\n","\n","Validation BLEU 9000: 18.9\n","Saved model: 9000 | BLEU 18.90\n","218979.87983226776 73562\n","Epoch  3,  9100/1000000; acc:  46.78; ppl:  19.63; 386 tgt tok/s;   1178 s elapsed\n","226945.45814466476 74933\n","Epoch  3,  9200/1000000; acc:  46.19; ppl:  20.67; 1604 tgt tok/s;   1225 s elapsed\n","228458.5353498459 75020\n","Epoch  3,  9300/1000000; acc:  45.83; ppl:  21.02; 1610 tgt tok/s;   1271 s elapsed\n","221093.8412413597 73977\n","Epoch  3,  9400/1000000; acc:  46.81; ppl:  19.86; 1588 tgt tok/s;   1318 s elapsed\n","216756.07039403915 72834\n","Epoch  3,  9500/1000000; acc:  46.94; ppl:  19.61; 1576 tgt tok/s;   1364 s elapsed\n","220170.11510658264 74064\n","Epoch  3,  9600/1000000; acc:  46.88; ppl:  19.54; 1562 tgt tok/s;   1411 s elapsed\n","221562.93598413467 74467\n","Epoch  3,  9700/1000000; acc:  46.87; ppl:  19.60; 1578 tgt tok/s;   1459 s elapsed\n","223928.7146267891 75000\n","Epoch  3,  9800/1000000; acc:  46.55; ppl:  19.80; 1580 tgt tok/s;   1506 s elapsed\n","224247.05566883087 75216\n","Epoch  3,  9900/1000000; acc:  46.78; ppl:  19.71; 1591 tgt tok/s;   1553 s elapsed\n","221318.90648937225 74171\n","Epoch  3, 10000/1000000; acc:  46.72; ppl:  19.76; 1577 tgt tok/s;   1600 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","69232.02305984497 22499\n","69232.02305984497 22499\n","Validation perplexity 10000: 21.6957\n","Learning rate: 0.001\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 18.41, 49.4/23.6/13.0/7.6 (BP=1.000, ratio=1.069, hyp_len=22295, ref_len=20850)\n","\n","Validation BLEU 10000: 18.41\n","217714.83622026443 73625\n","Epoch  3, 10100/1000000; acc:  47.21; ppl:  19.24; 385 tgt tok/s;   1792 s elapsed\n","219131.1303884983 73873\n","Epoch  3, 10200/1000000; acc:  46.86; ppl:  19.42; 1579 tgt tok/s;   1838 s elapsed\n","223979.78067207336 74598\n","Epoch  3, 10300/1000000; acc:  46.31; ppl:  20.14; 1588 tgt tok/s;   1885 s elapsed\n","220112.9436903 74622\n","Epoch  3, 10400/1000000; acc:  47.37; ppl:  19.10; 1587 tgt tok/s;   1932 s elapsed\n","221596.62846326828 74581\n","Epoch  3, 10500/1000000; acc:  47.07; ppl:  19.52; 1581 tgt tok/s;   1980 s elapsed\n","220324.6764769554 74432\n","Epoch  3, 10600/1000000; acc:  47.24; ppl:  19.30; 1581 tgt tok/s;   2027 s elapsed\n","218520.94448709488 74081\n","Epoch  3, 10700/1000000; acc:  47.21; ppl:  19.10; 1574 tgt tok/s;   2074 s elapsed\n","shuffle iwslt/train.vi.bpe | iwslt/train.en.bpe\n","39731.467935562134 13696\n","Epoch  4, 10800/1000000; acc:  47.50; ppl:  18.19; 1706 tgt tok/s;      8 s elapsed\n","204097.00004196167 73572\n","Epoch  4, 10900/1000000; acc:  48.94; ppl:  16.02; 1572 tgt tok/s;     55 s elapsed\n","208659.87001800537 74889\n","Epoch  4, 11000/1000000; acc:  48.66; ppl:  16.22; 1584 tgt tok/s;    102 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","68067.27612304688 22499\n","68067.27612304688 22499\n","Validation perplexity 11000: 20.6011\n","Learning rate: 0.001\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 19.67, 50.5/24.9/14.1/8.5 (BP=1.000, ratio=1.061, hyp_len=22122, ref_len=20850)\n","\n","Validation BLEU 11000: 19.67\n","Saved model: 11000 | BLEU 19.67\n","206545.34721565247 73790\n","Epoch  4, 11100/1000000; acc:  48.59; ppl:  16.43; 384 tgt tok/s;    294 s elapsed\n","205419.80097007751 74331\n","Epoch  4, 11200/1000000; acc:  49.00; ppl:  15.86; 1573 tgt tok/s;    342 s elapsed\n","205629.98948287964 73710\n","Epoch  4, 11300/1000000; acc:  48.72; ppl:  16.28; 1568 tgt tok/s;    389 s elapsed\n","211010.81055164337 75888\n","Epoch  4, 11400/1000000; acc:  48.92; ppl:  16.13; 1600 tgt tok/s;    436 s elapsed\n","210876.50542879105 75120\n","Epoch  4, 11500/1000000; acc:  48.61; ppl:  16.56; 1576 tgt tok/s;    484 s elapsed\n","207818.00114154816 74827\n","Epoch  4, 11600/1000000; acc:  49.08; ppl:  16.08; 1576 tgt tok/s;    531 s elapsed\n","216882.87668943405 76144\n","Epoch  4, 11700/1000000; acc:  47.88; ppl:  17.26; 1592 tgt tok/s;    579 s elapsed\n","202110.3917837143 73045\n","Epoch  4, 11800/1000000; acc:  49.20; ppl:  15.91; 1560 tgt tok/s;    626 s elapsed\n","208211.64889144897 74995\n","Epoch  4, 11900/1000000; acc:  49.02; ppl:  16.06; 1580 tgt tok/s;    673 s elapsed\n","205550.51252937317 74257\n","Epoch  4, 12000/1000000; acc:  48.97; ppl:  15.93; 1570 tgt tok/s;    721 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","67072.45152664185 22499\n","67072.45152664185 22499\n","Validation perplexity 12000: 19.7101\n","Learning rate: 0.001\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 19.57, 50.6/24.9/14.0/8.3 (BP=1.000, ratio=1.077, hyp_len=22457, ref_len=20850)\n","\n","Validation BLEU 12000: 19.57\n","215426.827170372 76061\n","Epoch  4, 12100/1000000; acc:  48.18; ppl:  16.98; 393 tgt tok/s;    914 s elapsed\n","200588.20472431183 73055\n","Epoch  4, 12200/1000000; acc:  49.34; ppl:  15.58; 1562 tgt tok/s;    961 s elapsed\n","206303.1032447815 73788\n","Epoch  4, 12300/1000000; acc:  48.74; ppl:  16.38; 1572 tgt tok/s;   1008 s elapsed\n","213515.82675361633 76475\n","Epoch  4, 12400/1000000; acc:  48.77; ppl:  16.31; 1595 tgt tok/s;   1056 s elapsed\n","205850.83956766129 73961\n","Epoch  4, 12500/1000000; acc:  48.85; ppl:  16.17; 1579 tgt tok/s;   1102 s elapsed\n","204345.66747665405 73692\n","Epoch  4, 12600/1000000; acc:  49.22; ppl:  16.01; 1562 tgt tok/s;   1150 s elapsed\n","212029.3104505539 76313\n","Epoch  4, 12700/1000000; acc:  48.89; ppl:  16.09; 1592 tgt tok/s;   1198 s elapsed\n","205782.60866689682 73991\n","Epoch  4, 12800/1000000; acc:  49.00; ppl:  16.14; 1577 tgt tok/s;   1244 s elapsed\n","204744.87638378143 73875\n","Epoch  4, 12900/1000000; acc:  49.25; ppl:  15.98; 1568 tgt tok/s;   1292 s elapsed\n","204355.7067861557 74077\n","Epoch  4, 13000/1000000; acc:  49.23; ppl:  15.78; 1572 tgt tok/s;   1339 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","65921.52908706665 22499\n","65921.52908706665 22499\n","Validation perplexity 13000: 18.7272\n","Learning rate: 0.001\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 19.75, 51.0/25.1/14.1/8.4 (BP=1.000, ratio=1.065, hyp_len=22196, ref_len=20850)\n","\n","Validation BLEU 13000: 19.75\n","Saved model: 13000 | BLEU 19.75\n","205029.67597198486 74060\n","Epoch  4, 13100/1000000; acc:  49.20; ppl:  15.93; 380 tgt tok/s;   1533 s elapsed\n","200871.7054080963 73089\n","Epoch  4, 13200/1000000; acc:  49.62; ppl:  15.62; 1566 tgt tok/s;   1580 s elapsed\n","207936.3390712738 75059\n","Epoch  4, 13300/1000000; acc:  48.95; ppl:  15.96; 1585 tgt tok/s;   1627 s elapsed\n","208790.97434139252 74972\n","Epoch  4, 13400/1000000; acc:  49.17; ppl:  16.20; 1585 tgt tok/s;   1675 s elapsed\n","205210.63777208328 73690\n","Epoch  4, 13500/1000000; acc:  48.78; ppl:  16.20; 1575 tgt tok/s;   1722 s elapsed\n","205370.83442687988 74084\n","Epoch  4, 13600/1000000; acc:  49.20; ppl:  15.99; 1566 tgt tok/s;   1769 s elapsed\n","207045.34829330444 74495\n","Epoch  4, 13700/1000000; acc:  49.05; ppl:  16.11; 1571 tgt tok/s;   1816 s elapsed\n","203552.14453792572 74375\n","Epoch  4, 13800/1000000; acc:  49.63; ppl:  15.44; 1568 tgt tok/s;   1864 s elapsed\n","206318.7814655304 74351\n","Epoch  4, 13900/1000000; acc:  48.88; ppl:  16.04; 1559 tgt tok/s;   1911 s elapsed\n","209384.3439488411 75121\n","Epoch  4, 14000/1000000; acc:  48.97; ppl:  16.24; 1576 tgt tok/s;   1959 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","65252.68832397461 22499\n","65252.68832397461 22499\n","Validation perplexity 14000: 18.1787\n","Learning rate: 0.001\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 20.85, 52.1/26.2/15.0/9.2 (BP=1.000, ratio=1.048, hyp_len=21856, ref_len=20850)\n","\n","Validation BLEU 14000: 20.85\n","Saved model: 14000 | BLEU 20.85\n","203590.23389482498 74755\n","Epoch  4, 14100/1000000; acc:  49.84; ppl:  15.23; 383 tgt tok/s;   2154 s elapsed\n","200929.35981941223 73499\n","Epoch  4, 14200/1000000; acc:  49.72; ppl:  15.39; 1561 tgt tok/s;   2201 s elapsed\n","198444.73932170868 72746\n","Epoch  4, 14300/1000000; acc:  49.76; ppl:  15.30; 1559 tgt tok/s;   2248 s elapsed\n","shuffle iwslt/train.vi.bpe | iwslt/train.en.bpe\n","37326.930908203125 14552\n","Epoch  5, 14400/1000000; acc:  50.96; ppl:  13.00; 1483 tgt tok/s;     10 s elapsed\n","191517.81583166122 74713\n","Epoch  5, 14500/1000000; acc:  51.34; ppl:  12.98; 1505 tgt tok/s;     59 s elapsed\n","191539.27907943726 74246\n","Epoch  5, 14600/1000000; acc:  51.07; ppl:  13.19; 1507 tgt tok/s;    109 s elapsed\n","189816.7341632843 73691\n","Epoch  5, 14700/1000000; acc:  51.19; ppl:  13.14; 1526 tgt tok/s;    157 s elapsed\n","193626.63245677948 75191\n","Epoch  5, 14800/1000000; acc:  51.38; ppl:  13.13; 1553 tgt tok/s;    205 s elapsed\n","193331.80244636536 74296\n","Epoch  5, 14900/1000000; acc:  50.87; ppl:  13.49; 1550 tgt tok/s;    253 s elapsed\n","194336.74996948242 74734\n","Epoch  5, 15000/1000000; acc:  51.01; ppl:  13.47; 1558 tgt tok/s;    301 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","65514.183029174805 22499\n","65514.183029174805 22499\n","Validation perplexity 15000: 18.3912\n","Learning rate: 0.0005\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 20.92, 52.7/26.4/15.1/9.1 (BP=1.000, ratio=1.040, hyp_len=21685, ref_len=20850)\n","\n","Validation BLEU 15000: 20.92\n","Saved model: 15000 | BLEU 20.92\n","190054.509516716 73833\n","Epoch  5, 15100/1000000; acc:  51.43; ppl:  13.12; 359 tgt tok/s;    507 s elapsed\n","188431.98798704147 73573\n","Epoch  5, 15200/1000000; acc:  51.61; ppl:  12.95; 1495 tgt tok/s;    556 s elapsed\n","183155.50108337402 72602\n","Epoch  5, 15300/1000000; acc:  51.89; ppl:  12.46; 1493 tgt tok/s;    605 s elapsed\n","188935.32039880753 74589\n","Epoch  5, 15400/1000000; acc:  52.13; ppl:  12.59; 1528 tgt tok/s;    654 s elapsed\n","187825.12592887878 73798\n","Epoch  5, 15500/1000000; acc:  51.84; ppl:  12.74; 1532 tgt tok/s;    702 s elapsed\n","194900.184820652 75887\n","Epoch  5, 15600/1000000; acc:  51.52; ppl:  13.04; 1556 tgt tok/s;    751 s elapsed\n","187124.85076904297 74614\n","Epoch  5, 15700/1000000; acc:  52.36; ppl:  12.28; 1542 tgt tok/s;    799 s elapsed\n","190609.84416246414 75646\n","Epoch  5, 15800/1000000; acc:  52.19; ppl:  12.43; 1553 tgt tok/s;    848 s elapsed\n","188875.23041296005 75236\n","Epoch  5, 15900/1000000; acc:  52.28; ppl:  12.31; 1549 tgt tok/s;    896 s elapsed\n","188576.29589509964 74966\n","Epoch  5, 16000/1000000; acc:  52.43; ppl:  12.37; 1545 tgt tok/s;    945 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","63366.742767333984 22499\n","63366.742767333984 22499\n","Validation perplexity 16000: 16.717\n","Learning rate: 0.0005\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 22.18, 53.8/27.8/16.2/10.0 (BP=1.000, ratio=1.041, hyp_len=21698, ref_len=20850)\n","\n","Validation BLEU 16000: 22.18\n","Saved model: 16000 | BLEU 22.18\n","185394.73706817627 74227\n","Epoch  5, 16100/1000000; acc:  52.76; ppl:  12.15; 379 tgt tok/s;   1141 s elapsed\n","190217.2284603119 76185\n","Epoch  5, 16200/1000000; acc:  52.49; ppl:  12.14; 1579 tgt tok/s;   1189 s elapsed\n","183521.24095249176 74125\n","Epoch  5, 16300/1000000; acc:  52.46; ppl:  11.89; 1542 tgt tok/s;   1237 s elapsed\n","191038.02190876007 75846\n","Epoch  5, 16400/1000000; acc:  52.10; ppl:  12.41; 1565 tgt tok/s;   1286 s elapsed\n","184661.25142478943 74229\n","Epoch  5, 16500/1000000; acc:  52.48; ppl:  12.03; 1544 tgt tok/s;   1334 s elapsed\n","181748.9477376938 72847\n","Epoch  5, 16600/1000000; acc:  52.34; ppl:  12.12; 1534 tgt tok/s;   1381 s elapsed\n","181267.02064323425 72826\n","Epoch  5, 16700/1000000; acc:  52.69; ppl:  12.05; 1524 tgt tok/s;   1429 s elapsed\n","183392.900639534 73578\n","Epoch  5, 16800/1000000; acc:  52.50; ppl:  12.09; 1537 tgt tok/s;   1477 s elapsed\n","182700.7376241684 74123\n","Epoch  5, 16900/1000000; acc:  52.91; ppl:  11.76; 1554 tgt tok/s;   1524 s elapsed\n","186116.78720760345 75292\n","Epoch  5, 17000/1000000; acc:  52.73; ppl:  11.85; 1559 tgt tok/s;   1573 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","62771.190046310425 22499\n","62771.190046310425 22499\n","Validation perplexity 17000: 16.2803\n","Learning rate: 0.0005\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 21.95, 53.6/27.5/16.1/9.8 (BP=1.000, ratio=1.040, hyp_len=21678, ref_len=20850)\n","\n","Validation BLEU 17000: 21.95\n","188643.42360973358 75170\n","Epoch  5, 17100/1000000; acc:  52.35; ppl:  12.30; 388 tgt tok/s;   1766 s elapsed\n","180298.98001861572 73255\n","Epoch  5, 17200/1000000; acc:  53.14; ppl:  11.72; 1529 tgt tok/s;   1814 s elapsed\n","186885.13310432434 75346\n","Epoch  5, 17300/1000000; acc:  52.83; ppl:  11.95; 1564 tgt tok/s;   1862 s elapsed\n","180802.02706718445 73820\n","Epoch  5, 17400/1000000; acc:  53.14; ppl:  11.58; 1540 tgt tok/s;   1910 s elapsed\n","182082.33015298843 74413\n","Epoch  5, 17500/1000000; acc:  53.20; ppl:  11.55; 1556 tgt tok/s;   1958 s elapsed\n","187830.1816754341 75467\n","Epoch  5, 17600/1000000; acc:  52.74; ppl:  12.05; 1564 tgt tok/s;   2006 s elapsed\n","184174.755859375 74276\n","Epoch  5, 17700/1000000; acc:  52.84; ppl:  11.94; 1542 tgt tok/s;   2055 s elapsed\n","180709.07379817963 73522\n","Epoch  5, 17800/1000000; acc:  53.16; ppl:  11.68; 1546 tgt tok/s;   2102 s elapsed\n","183067.96572971344 74182\n","Epoch  5, 17900/1000000; acc:  52.72; ppl:  11.80; 1546 tgt tok/s;   2150 s elapsed\n","shuffle iwslt/train.vi.bpe | iwslt/train.en.bpe\n","55672.27621746063 22514\n","Epoch  6, 18000/1000000; acc:  52.55; ppl:  11.86; 1646 tgt tok/s;     14 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","62479.689306259155 22499\n","62479.689306259155 22499\n","Validation perplexity 18000: 16.0707\n","Learning rate: 0.0005\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 22.23, 54.3/27.9/16.2/10.0 (BP=1.000, ratio=1.035, hyp_len=21574, ref_len=20850)\n","\n","Validation BLEU 18000: 22.23\n","Saved model: 18000 | BLEU 22.23\n","174270.38485336304 75031\n","Epoch  6, 18100/1000000; acc:  54.79; ppl:  10.20; 386 tgt tok/s;    208 s elapsed\n","172772.7196779251 74090\n","Epoch  6, 18200/1000000; acc:  54.33; ppl:  10.30; 1547 tgt tok/s;    256 s elapsed\n","175569.22130012512 74681\n","Epoch  6, 18300/1000000; acc:  54.41; ppl:  10.50; 1559 tgt tok/s;    304 s elapsed\n","175083.7165145874 74835\n","Epoch  6, 18400/1000000; acc:  54.44; ppl:  10.38; 1554 tgt tok/s;    352 s elapsed\n","181520.43051195145 76541\n","Epoch  6, 18500/1000000; acc:  54.18; ppl:  10.71; 1571 tgt tok/s;    401 s elapsed\n","174259.26794338226 74614\n","Epoch  6, 18600/1000000; acc:  54.38; ppl:  10.33; 1554 tgt tok/s;    449 s elapsed\n","172278.327917099 74283\n","Epoch  6, 18700/1000000; acc:  54.74; ppl:  10.17; 1550 tgt tok/s;    496 s elapsed\n","178009.66722631454 75691\n","Epoch  6, 18800/1000000; acc:  54.12; ppl:  10.50; 1571 tgt tok/s;    545 s elapsed\n","173006.0599937439 74095\n","Epoch  6, 18900/1000000; acc:  54.49; ppl:  10.33; 1554 tgt tok/s;    592 s elapsed\n","177499.93760490417 74873\n","Epoch  6, 19000/1000000; acc:  54.15; ppl:  10.70; 1567 tgt tok/s;    640 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","62460.35810852051 22499\n","62460.35810852051 22499\n","Validation perplexity 19000: 16.0569\n","Learning rate: 0.0005\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 21.98, 53.9/27.7/16.1/9.8 (BP=1.000, ratio=1.032, hyp_len=21526, ref_len=20850)\n","\n","Validation BLEU 19000: 21.98\n","178286.4576883316 75741\n","Epoch  6, 19100/1000000; acc:  53.92; ppl:  10.53; 392 tgt tok/s;    833 s elapsed\n","175852.9885582924 73989\n","Epoch  6, 19200/1000000; acc:  53.96; ppl:  10.77; 1546 tgt tok/s;    881 s elapsed\n","170598.51735973358 72968\n","Epoch  6, 19300/1000000; acc:  54.39; ppl:  10.36; 1530 tgt tok/s;    929 s elapsed\n","175196.71780633926 73944\n","Epoch  6, 19400/1000000; acc:  53.96; ppl:  10.69; 1536 tgt tok/s;    977 s elapsed\n","171981.95606040955 73349\n","Epoch  6, 19500/1000000; acc:  54.36; ppl:  10.43; 1543 tgt tok/s;   1025 s elapsed\n","179006.41130065918 75447\n","Epoch  6, 19600/1000000; acc:  53.99; ppl:  10.73; 1563 tgt tok/s;   1073 s elapsed\n","175626.5161819458 74889\n","Epoch  6, 19700/1000000; acc:  54.31; ppl:  10.43; 1564 tgt tok/s;   1121 s elapsed\n","172902.3504304886 73542\n","Epoch  6, 19800/1000000; acc:  54.41; ppl:  10.50; 1544 tgt tok/s;   1168 s elapsed\n","174045.66469192505 73478\n","Epoch  6, 19900/1000000; acc:  54.12; ppl:  10.68; 1541 tgt tok/s;   1216 s elapsed\n","176248.51103305817 74491\n","Epoch  6, 20000/1000000; acc:  54.01; ppl:  10.66; 1554 tgt tok/s;   1264 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","61920.55739593506 22499\n","61920.55739593506 22499\n","Validation perplexity 20000: 15.6763\n","Learning rate: 0.0005\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 22.64, 54.6/28.3/16.5/10.3 (BP=1.000, ratio=1.021, hyp_len=21290, ref_len=20850)\n","\n","Validation BLEU 20000: 22.64\n","Saved model: 20000 | BLEU 22.64\n","172877.30951023102 74006\n","Epoch  6, 20100/1000000; acc:  54.71; ppl:  10.34; 380 tgt tok/s;   1459 s elapsed\n","172986.8530845642 73813\n","Epoch  6, 20200/1000000; acc:  54.39; ppl:  10.42; 1549 tgt tok/s;   1506 s elapsed\n","176782.00629138947 74848\n","Epoch  6, 20300/1000000; acc:  54.18; ppl:  10.61; 1538 tgt tok/s;   1555 s elapsed\n","172665.71748924255 74211\n","Epoch  6, 20400/1000000; acc:  54.50; ppl:  10.24; 1534 tgt tok/s;   1603 s elapsed\n","173333.5670261383 74359\n","Epoch  6, 20500/1000000; acc:  54.76; ppl:  10.29; 1522 tgt tok/s;   1652 s elapsed\n","175581.03524971008 74610\n","Epoch  6, 20600/1000000; acc:  54.21; ppl:  10.52; 1512 tgt tok/s;   1702 s elapsed\n","174847.1338968277 74877\n","Epoch  6, 20700/1000000; acc:  54.69; ppl:  10.33; 1514 tgt tok/s;   1751 s elapsed\n","172308.3814725876 73763\n","Epoch  6, 20800/1000000; acc:  54.62; ppl:  10.34; 1500 tgt tok/s;   1800 s elapsed\n","173964.8451142311 74162\n","Epoch  6, 20900/1000000; acc:  54.43; ppl:  10.44; 1508 tgt tok/s;   1849 s elapsed\n","171730.36211967468 73483\n","Epoch  6, 21000/1000000; acc:  54.42; ppl:  10.35; 1508 tgt tok/s;   1898 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","61680.15181541443 22499\n","61680.15181541443 22499\n","Validation perplexity 21000: 15.5096\n","Learning rate: 0.0005\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 22.52, 54.2/28.2/16.5/10.2 (BP=1.000, ratio=1.039, hyp_len=21655, ref_len=20850)\n","\n","Validation BLEU 21000: 22.52\n","173883.17757463455 73692\n","Epoch  6, 21100/1000000; acc:  54.31; ppl:  10.59; 369 tgt tok/s;   2098 s elapsed\n","172907.59192037582 73443\n","Epoch  6, 21200/1000000; acc:  54.33; ppl:  10.53; 1521 tgt tok/s;   2146 s elapsed\n","174577.02105617523 74521\n","Epoch  6, 21300/1000000; acc:  54.62; ppl:  10.41; 1521 tgt tok/s;   2195 s elapsed\n","174448.58187961578 74498\n","Epoch  6, 21400/1000000; acc:  54.54; ppl:  10.40; 1518 tgt tok/s;   2244 s elapsed\n","176872.76537704468 74983\n","Epoch  6, 21500/1000000; acc:  54.09; ppl:  10.58; 1521 tgt tok/s;   2294 s elapsed\n","shuffle iwslt/train.vi.bpe | iwslt/train.en.bpe\n","59825.76918506622 26589\n","Epoch  7, 21600/1000000; acc:  54.93; ppl:   9.49; 1645 tgt tok/s;     16 s elapsed\n","163530.4739561081 73876\n","Epoch  7, 21700/1000000; acc:  56.00; ppl:   9.15; 1514 tgt tok/s;     65 s elapsed\n","165404.60337734222 74810\n","Epoch  7, 21800/1000000; acc:  55.97; ppl:   9.12; 1529 tgt tok/s;    114 s elapsed\n","160720.8943309784 73238\n","Epoch  7, 21900/1000000; acc:  56.41; ppl:   8.98; 1514 tgt tok/s;    162 s elapsed\n","165493.02274131775 74382\n","Epoch  7, 22000/1000000; acc:  55.82; ppl:   9.25; 1528 tgt tok/s;    211 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","61855.008922576904 22499\n","61855.008922576904 22499\n","Validation perplexity 22000: 15.6306\n","Learning rate: 0.00025\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 22.36, 54.3/28.0/16.3/10.1 (BP=1.000, ratio=1.036, hyp_len=21592, ref_len=20850)\n","\n","Validation BLEU 22000: 22.36\n","167066.33332824707 74374\n","Epoch  7, 22100/1000000; acc:  55.67; ppl:   9.45; 372 tgt tok/s;    411 s elapsed\n","161041.02910852432 73170\n","Epoch  7, 22200/1000000; acc:  56.27; ppl:   9.03; 1518 tgt tok/s;    459 s elapsed\n","161970.43161010742 73778\n","Epoch  7, 22300/1000000; acc:  56.43; ppl:   8.98; 1521 tgt tok/s;    508 s elapsed\n","166532.7684583664 75242\n","Epoch  7, 22400/1000000; acc:  56.15; ppl:   9.15; 1512 tgt tok/s;    557 s elapsed\n","167194.71982097626 74770\n","Epoch  7, 22500/1000000; acc:  55.79; ppl:   9.36; 1516 tgt tok/s;    607 s elapsed\n","162573.99757003784 74003\n","Epoch  7, 22600/1000000; acc:  56.62; ppl:   9.00; 1515 tgt tok/s;    655 s elapsed\n","165967.47414064407 75548\n","Epoch  7, 22700/1000000; acc:  56.30; ppl:   9.00; 1546 tgt tok/s;    704 s elapsed\n","167582.7322230339 74934\n","Epoch  7, 22800/1000000; acc:  55.84; ppl:   9.36; 1536 tgt tok/s;    753 s elapsed\n","164969.29714107513 74478\n","Epoch  7, 22900/1000000; acc:  55.98; ppl:   9.16; 1540 tgt tok/s;    801 s elapsed\n","160871.39021205902 74127\n","Epoch  7, 23000/1000000; acc:  56.66; ppl:   8.76; 1526 tgt tok/s;    850 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","61286.74907684326 22499\n","61286.74907684326 22499\n","Validation perplexity 23000: 15.2408\n","Learning rate: 0.00025\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 22.54, 54.2/28.2/16.5/10.2 (BP=1.000, ratio=1.040, hyp_len=21674, ref_len=20850)\n","\n","Validation BLEU 23000: 22.54\n","164504.2770729065 74089\n","Epoch  7, 23100/1000000; acc:  56.04; ppl:   9.21; 370 tgt tok/s;   1050 s elapsed\n","161627.6240158081 74025\n","Epoch  7, 23200/1000000; acc:  56.79; ppl:   8.88; 1516 tgt tok/s;   1099 s elapsed\n","158596.27357959747 73453\n","Epoch  7, 23300/1000000; acc:  56.75; ppl:   8.66; 1507 tgt tok/s;   1148 s elapsed\n","167178.1407160759 75632\n","Epoch  7, 23400/1000000; acc:  56.00; ppl:   9.12; 1552 tgt tok/s;   1197 s elapsed\n","163935.52693080902 74501\n","Epoch  7, 23500/1000000; acc:  56.37; ppl:   9.03; 1546 tgt tok/s;   1245 s elapsed\n","167876.00398731232 74430\n","Epoch  7, 23600/1000000; acc:  55.57; ppl:   9.54; 1556 tgt tok/s;   1293 s elapsed\n","165075.8563694954 75333\n","Epoch  7, 23700/1000000; acc:  56.52; ppl:   8.95; 1574 tgt tok/s;   1341 s elapsed\n","162658.3519964218 74123\n","Epoch  7, 23800/1000000; acc:  56.69; ppl:   8.97; 1552 tgt tok/s;   1388 s elapsed\n","159959.46487522125 73275\n","Epoch  7, 23900/1000000; acc:  56.36; ppl:   8.87; 1548 tgt tok/s;   1436 s elapsed\n","165870.78308296204 74761\n","Epoch  7, 24000/1000000; acc:  56.15; ppl:   9.20; 1558 tgt tok/s;   1484 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","60919.621894836426 22499\n","60919.621894836426 22499\n","Validation perplexity 24000: 14.9941\n","Learning rate: 0.00025\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 22.87, 54.6/28.6/16.8/10.4 (BP=1.000, ratio=1.040, hyp_len=21677, ref_len=20850)\n","\n","Validation BLEU 24000: 22.87\n","Saved model: 24000 | BLEU 22.87\n","165565.42553567886 75638\n","Epoch  7, 24100/1000000; acc:  56.45; ppl:   8.93; 398 tgt tok/s;   1673 s elapsed\n","162281.4342007637 74440\n","Epoch  7, 24200/1000000; acc:  56.62; ppl:   8.85; 1579 tgt tok/s;   1721 s elapsed\n","162517.65061569214 73743\n","Epoch  7, 24300/1000000; acc:  56.55; ppl:   9.06; 1579 tgt tok/s;   1767 s elapsed\n","166951.6588420868 75169\n","Epoch  7, 24400/1000000; acc:  56.15; ppl:   9.22; 1584 tgt tok/s;   1815 s elapsed\n","167395.70378398895 75203\n","Epoch  7, 24500/1000000; acc:  55.89; ppl:   9.26; 1586 tgt tok/s;   1862 s elapsed\n","161195.2972574234 72788\n","Epoch  7, 24600/1000000; acc:  56.25; ppl:   9.16; 1573 tgt tok/s;   1908 s elapsed\n","164021.9449968338 74697\n","Epoch  7, 24700/1000000; acc:  56.40; ppl:   8.99; 1579 tgt tok/s;   1956 s elapsed\n","165424.88180303574 75198\n","Epoch  7, 24800/1000000; acc:  56.36; ppl:   9.02; 1596 tgt tok/s;   2003 s elapsed\n","162184.4348897934 74507\n","Epoch  7, 24900/1000000; acc:  56.60; ppl:   8.82; 1582 tgt tok/s;   2050 s elapsed\n","162562.01059865952 74491\n","Epoch  7, 25000/1000000; acc:  56.53; ppl:   8.87; 1579 tgt tok/s;   2097 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","60778.691020965576 22499\n","60778.691020965576 22499\n","Validation perplexity 25000: 14.9005\n","Learning rate: 0.00025\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 23.10, 55.2/28.9/17.0/10.5 (BP=1.000, ratio=1.028, hyp_len=21431, ref_len=20850)\n","\n","Validation BLEU 25000: 23.1\n","Saved model: 25000 | BLEU 23.10\n","161578.79969024658 73722\n","Epoch  7, 25100/1000000; acc:  56.54; ppl:   8.95; 390 tgt tok/s;   2286 s elapsed\n","shuffle iwslt/train.vi.bpe | iwslt/train.en.bpe\n","62719.99238491058 28334\n","Epoch  8, 25200/1000000; acc:  55.92; ppl:   9.15; 1638 tgt tok/s;     17 s elapsed\n","158522.22463035583 75189\n","Epoch  8, 25300/1000000; acc:  57.34; ppl:   8.23; 1590 tgt tok/s;     65 s elapsed\n","157903.5349407196 74318\n","Epoch  8, 25400/1000000; acc:  57.46; ppl:   8.37; 1583 tgt tok/s;    112 s elapsed\n","156222.33617162704 74151\n","Epoch  8, 25500/1000000; acc:  57.78; ppl:   8.22; 1588 tgt tok/s;    158 s elapsed\n","154535.4631204605 74219\n","Epoch  8, 25600/1000000; acc:  57.86; ppl:   8.02; 1589 tgt tok/s;    205 s elapsed\n","160398.822142601 75185\n","Epoch  8, 25700/1000000; acc:  57.45; ppl:   8.44; 1593 tgt tok/s;    252 s elapsed\n","159811.55826854706 74804\n","Epoch  8, 25800/1000000; acc:  57.30; ppl:   8.47; 1593 tgt tok/s;    299 s elapsed\n","159292.1200594902 74820\n","Epoch  8, 25900/1000000; acc:  57.14; ppl:   8.41; 1589 tgt tok/s;    346 s elapsed\n","156537.32321596146 75025\n","Epoch  8, 26000/1000000; acc:  57.85; ppl:   8.06; 1592 tgt tok/s;    393 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","61163.99653625488 22499\n","61163.99653625488 22499\n","Validation perplexity 26000: 15.1579\n","Learning rate: 0.000125\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 22.78, 54.3/28.4/16.7/10.5 (BP=1.000, ratio=1.046, hyp_len=21816, ref_len=20850)\n","\n","Validation BLEU 26000: 22.78\n","152957.7508020401 72957\n","Epoch  8, 26100/1000000; acc:  57.89; ppl:   8.14; 393 tgt tok/s;    579 s elapsed\n","155096.28682231903 74002\n","Epoch  8, 26200/1000000; acc:  57.67; ppl:   8.13; 1592 tgt tok/s;    626 s elapsed\n","160153.04376792908 75189\n","Epoch  8, 26300/1000000; acc:  57.21; ppl:   8.41; 1604 tgt tok/s;    673 s elapsed\n","161087.8678164482 75926\n","Epoch  8, 26400/1000000; acc:  57.60; ppl:   8.34; 1607 tgt tok/s;    720 s elapsed\n","158082.68316221237 74454\n","Epoch  8, 26500/1000000; acc:  57.11; ppl:   8.36; 1589 tgt tok/s;    767 s elapsed\n","155073.1921172142 74648\n","Epoch  8, 26600/1000000; acc:  58.23; ppl:   7.98; 1602 tgt tok/s;    813 s elapsed\n","154807.09050750732 74308\n","Epoch  8, 26700/1000000; acc:  57.96; ppl:   8.03; 1592 tgt tok/s;    860 s elapsed\n","153899.48955631256 73745\n","Epoch  8, 26800/1000000; acc:  58.04; ppl:   8.06; 1591 tgt tok/s;    906 s elapsed\n","157428.58355236053 75162\n","Epoch  8, 26900/1000000; acc:  57.74; ppl:   8.12; 1607 tgt tok/s;    953 s elapsed\n","160758.3217997551 75334\n","Epoch  8, 27000/1000000; acc:  57.20; ppl:   8.45; 1606 tgt tok/s;   1000 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","60782.130599975586 22499\n","60782.130599975586 22499\n","Validation perplexity 27000: 14.9028\n","Learning rate: 6.25e-05\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 22.81, 54.3/28.6/16.7/10.4 (BP=1.000, ratio=1.046, hyp_len=21818, ref_len=20850)\n","\n","Validation BLEU 27000: 22.81\n","156144.83490085602 74397\n","Epoch  8, 27100/1000000; acc:  57.89; ppl:   8.16; 400 tgt tok/s;   1186 s elapsed\n","158989.97145748138 75584\n","Epoch  8, 27200/1000000; acc:  57.75; ppl:   8.19; 1607 tgt tok/s;   1233 s elapsed\n","153474.0902247429 74100\n","Epoch  8, 27300/1000000; acc:  58.18; ppl:   7.93; 1594 tgt tok/s;   1279 s elapsed\n","158171.8605041504 74702\n","Epoch  8, 27400/1000000; acc:  57.64; ppl:   8.31; 1600 tgt tok/s;   1326 s elapsed\n","158173.58988666534 75070\n","Epoch  8, 27500/1000000; acc:  57.76; ppl:   8.22; 1599 tgt tok/s;   1373 s elapsed\n","155526.05092668533 74966\n","Epoch  8, 27600/1000000; acc:  57.85; ppl:   7.96; 1597 tgt tok/s;   1420 s elapsed\n","156245.69013023376 74421\n","Epoch  8, 27700/1000000; acc:  57.88; ppl:   8.16; 1591 tgt tok/s;   1467 s elapsed\n","159129.80562734604 74855\n","Epoch  8, 27800/1000000; acc:  57.24; ppl:   8.38; 1611 tgt tok/s;   1513 s elapsed\n","153880.34149312973 73514\n","Epoch  8, 27900/1000000; acc:  57.55; ppl:   8.11; 1585 tgt tok/s;   1560 s elapsed\n","152281.42326450348 73433\n","Epoch  8, 28000/1000000; acc:  58.14; ppl:   7.95; 1588 tgt tok/s;   1606 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","60657.56351852417 22499\n","60657.56351852417 22499\n","Validation perplexity 28000: 14.8205\n","Learning rate: 6.25e-05\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 23.36, 54.8/28.9/17.2/10.9 (BP=1.000, ratio=1.039, hyp_len=21666, ref_len=20850)\n","\n","Validation BLEU 28000: 23.36\n","Saved model: 28000 | BLEU 23.36\n","150059.77950382233 73167\n","Epoch  8, 28100/1000000; acc:  58.80; ppl:   7.78; 391 tgt tok/s;   1793 s elapsed\n","154212.53237247467 73640\n","Epoch  8, 28200/1000000; acc:  57.93; ppl:   8.12; 1588 tgt tok/s;   1839 s elapsed\n","151744.4415254593 73080\n","Epoch  8, 28300/1000000; acc:  58.23; ppl:   7.98; 1571 tgt tok/s;   1886 s elapsed\n","150830.1668958664 73446\n","Epoch  8, 28400/1000000; acc:  58.33; ppl:   7.80; 1577 tgt tok/s;   1932 s elapsed\n","155629.59914016724 74437\n","Epoch  8, 28500/1000000; acc:  57.84; ppl:   8.09; 1581 tgt tok/s;   1979 s elapsed\n","154754.43593978882 74109\n","Epoch  8, 28600/1000000; acc:  57.89; ppl:   8.07; 1593 tgt tok/s;   2026 s elapsed\n","155414.34525823593 74224\n","Epoch  8, 28700/1000000; acc:  58.01; ppl:   8.12; 1586 tgt tok/s;   2073 s elapsed\n","shuffle iwslt/train.vi.bpe | iwslt/train.en.bpe\n","59247.63594818115 28998\n","Epoch  9, 28800/1000000; acc:  58.59; ppl:   7.71; 1560 tgt tok/s;     19 s elapsed\n","154277.285531044 74848\n","Epoch  9, 28900/1000000; acc:  58.26; ppl:   7.86; 1601 tgt tok/s;     65 s elapsed\n","151982.83484601974 74900\n","Epoch  9, 29000/1000000; acc:  58.77; ppl:   7.61; 1594 tgt tok/s;    112 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","60829.40766906738 22499\n","60829.40766906738 22499\n","Validation perplexity 29000: 14.9341\n","Learning rate: 3.125e-05\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","MY_BLEU BLEU = 22.96, 54.4/28.6/16.9/10.6 (BP=1.000, ratio=1.049, hyp_len=21871, ref_len=20850)\n","\n","Validation BLEU 29000: 22.96\n","148993.98184251785 73275\n","Epoch  9, 29100/1000000; acc:  58.76; ppl:   7.64; 391 tgt tok/s;    300 s elapsed\n","148013.65720176697 73144\n","Epoch  9, 29200/1000000; acc:  58.91; ppl:   7.57; 1571 tgt tok/s;    346 s elapsed\n","150229.09862613678 74233\n","Epoch  9, 29300/1000000; acc:  58.81; ppl:   7.57; 1598 tgt tok/s;    393 s elapsed\n","152227.15758514404 73686\n","Epoch  9, 29400/1000000; acc:  58.04; ppl:   7.89; 1593 tgt tok/s;    439 s elapsed\n","148825.159760952 73703\n","Epoch  9, 29500/1000000; acc:  58.78; ppl:   7.53; 1584 tgt tok/s;    485 s elapsed\n","147502.21989154816 73110\n","Epoch  9, 29600/1000000; acc:  59.07; ppl:   7.52; 1586 tgt tok/s;    532 s elapsed\n","151841.61372804642 74299\n","Epoch  9, 29700/1000000; acc:  58.37; ppl:   7.72; 1597 tgt tok/s;    578 s elapsed\n","150340.22617721558 74190\n","Epoch  9, 29800/1000000; acc:  58.70; ppl:   7.59; 1591 tgt tok/s;    625 s elapsed\n","149645.66641139984 74713\n","Epoch  9, 29900/1000000; acc:  59.18; ppl:   7.41; 1605 tgt tok/s;    671 s elapsed\n","155581.71760702133 75612\n","Epoch  9, 30000/1000000; acc:  58.42; ppl:   7.83; 1612 tgt tok/s;    718 s elapsed\n","shuffle iwslt/test2015.vi.bpe | iwslt/test2015.en.bpe\n","60792.62480545044 22499\n","60792.62480545044 22499\n","Validation perplexity 30000: 14.9097\n","Learning rate: 1.5625e-05\n","Reaching minimum learning rate. Stop training!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sKxL_QoGFFiv","colab_type":"text"},"source":["USE MODEL TO TRANSLATE\n","# **TRANSLATE: VN-ENG**"]},{"cell_type":"markdown","metadata":{"id":"byjFDbbrGWvc","colab_type":"text"},"source":["create input.txt containing source sentences in Vietnamse"]},{"cell_type":"code","metadata":{"id":"lmyBRYzCGV56","colab_type":"code","outputId":"a9890d57-80b5-4147-88bf-0c20ea5253e6","executionInfo":{"status":"ok","timestamp":1578497130048,"user_tz":-420,"elapsed":1407,"user":{"displayName":"Phúc Trần Bảo","photoUrl":"","userId":"04637064817265378505"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile input.txt\n","Và ba mẹ con họ cứ thế reo lên mà nhạo báng đủ điều, sau đó mới đẩy cô xuống nhà bếp. Họ bắt cô từ sáng tới tận tối làm lụng vất vả, từ tờ mờ sáng thì cô đã phải dậy, rồi đi gánh nước, về lại nhóm bếp, sau đó thì thổi cơm và giặt giũ."],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting input.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Imn8aLMkKkCN","colab_type":"text"},"source":["translate command"]},{"cell_type":"code","metadata":{"id":"XMcIrrrHB3ra","colab_type":"code","outputId":"23173c7b-77f4-4bb7-e1fc-4b59272c9c27","executionInfo":{"status":"ok","timestamp":1578497218848,"user_tz":-420,"elapsed":9388,"user":{"displayName":"Phúc Trần Bảo","photoUrl":"","userId":"04637064817265378505"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["!python translate.py -checkpoint model_best.pt -input input.txt -gpuid 0"],"execution_count":0,"outputs":[{"output_type":"stream","text":["| train configuration\n","Namespace(batch_size=64, beam_size=12, datasets=['iwslt/train.vi.bpe', 'iwslt/train.en.bpe'], decoder_type='rnn', dicts=['iwslt/all.vi.bpe.pkl', 'iwslt/all.en.bpe.pkl'], dropout=0.3, encode_multi_key=True, encoder_type='sabrnn', epochs=13, eval_every=5000, gpuid=[0], hard=False, input_feed=1, layers=2, learning_rate=0.001, learning_rate_decay=0.5, max_decay_step=5, max_generator_batches=32, max_grad_norm=5, max_seq_length=100, max_thres=7.0, max_updates=1000000, min_thres=-5.0, optim='adam', param_init=0.1, report_every=100, rnn_size=256, rnn_type='LSTM', save_model='model', seed=42, share_attn=True, share_decoder_embeddings=True, shuffle=1, src_vocab_size=9759, start_checkpoint_at=0, start_epoch=1, start_eval_checkpoint_at=0, tgt_vocab_size=9973, train_from='model_best.pt', valid_datasets=['iwslt/test2015.vi.bpe', 'iwslt/test2015.en.bpe'], word_vec_size=256)\n","Loading model\n","MY OPT INPUT input.txt\n","/content/drive/My Drive/nlp_gpu_sa_nmt/Code/sa-nmt/infer.py:37: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  volatile=True)\n","/content/drive/My Drive/nlp_gpu_sa_nmt/Code/sa-nmt/infer.py:64: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  init_target = Variable(init_target, volatile=True)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/content/drive/My Drive/nlp_gpu_sa_nmt/Code/sa-nmt/infer.py:115: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  init_target = Variable(next_ys.view(1, -1), volatile=True)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zTn8u1OjGNMG","colab_type":"text"},"source":["VIEW TRANSLATE RESULT\n","# **VIEW TRANSLATION RESULT: VN-ENG**"]},{"cell_type":"code","metadata":{"id":"dlk7JQkCFV3a","colab_type":"code","colab":{}},"source":["%pycat trans.bpe"],"execution_count":0,"outputs":[]}]}